{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (2.7.1)\r\nRequirement already satisfied: torchvision in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (0.22.1)\r\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (11.3.0.4)\r\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (11.7.1.2)\r\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.85)\r\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.80)\r\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.5.4.2)\r\nRequirement already satisfied: typing-extensions>=4.10.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (4.14.1)\r\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (3.18.0)\r\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.77)\r\nRequirement already satisfied: sympy>=1.13.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (1.13.3)\r\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.4.1)\r\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (2023.10.0)\r\nRequirement already satisfied: triton==3.3.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (3.3.1)\r\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (9.5.1.17)\r\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (0.6.3)\r\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (3.1.6)\r\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.77)\r\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (2.26.2)\r\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (10.3.7.77)\r\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (1.11.1.6)\r\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (3.4.2)\r\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torch) (12.6.77)\r\nRequirement already satisfied: setuptools>=40.8.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from triton==3.3.1->torch) (75.8.0)\r\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torchvision) (1.23.5)\r\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from torchvision) (9.2.0)\r\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jinja2->torch) (2.0.1)\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metadata file (adjust path based on your folder)\n",
        "metadata_path = 'UrbanSound8K/metadata/UrbanSound8K.csv'\n",
        "df = pd.read_csv(metadata_path)\n",
        "print(df.head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n\n              class  \n0          dog_bark  \n1  children_playing  \n2  children_playing  \n3  children_playing  \n4  children_playing  \n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1753700002414
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data transformations\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224 for ResNet\n",
        "    transforms.ToTensor(),          # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Load the dataset from the spectrograms folder\n",
        "dataset = datasets.ImageFolder('spectrograms/', transform=data_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Print dataset info\n",
        "print(f\"Number of samples: {len(dataset)}\")\n",
        "print(f\"Classes: {dataset.classes}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of samples: 8732\nClasses: ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1753700005854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Data setup (from previous steps)\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder('spectrograms/', transform=data_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)  # Use 4 workers for 4 cores\n",
        "\n",
        "# Model and training (from previous steps)\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "device = torch.device('cpu')  # Force CPU for this test\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(dataloader, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Time the run\n",
        "start_time = time.time()\n",
        "train_model(dataloader)\n",
        "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1/5, Loss: 0.7015\nEpoch 2/5, Loss: 0.3400\nEpoch 3/5, Loss: 0.2331\nEpoch 4/5, Loss: 0.1691\nEpoch 5/5, Loss: 0.1351\nTime taken: 1772.34 seconds\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1753702794364
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.5.1)\r\nRequirement already satisfied: scipy>=1.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (1.11.0)\r\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\r\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\nRequirement already satisfied: numpy>=1.19.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\r\n"
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Run\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Get the current run context (this works in an Azure ML experiment)\n",
        "run = Run.get_context()\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Compute basic metrics\n",
        "accuracy = np.mean(all_preds == all_labels)\n",
        "run.log('Accuracy', accuracy)\n",
        "\n",
        "# Log confusion matrix (as a flattened array for now)\n",
        "confusion = np.zeros((10, 10), dtype=int)  # 10 classes in UrbanSound8K\n",
        "for i in range(len(all_labels)):\n",
        "    confusion[all_labels[i]][all_preds[i]] += 1\n",
        "run.log_table('Confusion Matrix', {'row': range(10), 'col': range(10), 'value': confusion.flatten()})\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix (logged to Azure ML run):\")\n",
        "print(confusion)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric Accuracy:\n0.9636967475950526\nAttempted to log table metric Confusion Matrix:\n{'row': range(0, 10), 'col': range(0, 10), 'value': array([970,   0,   2,   1,   4,   9,   0,   2,   6,   6,   0, 425,   0,\n         3,   0,   0,   0,   0,   0,   1,   0,   0, 872,  47,   0,   1,\n         0,   0,   0,  80,   1,   0,   0, 988,   1,   3,   0,   0,   1,\n         6,   3,   0,   0,  26, 945,   3,   0,  22,   0,   1,   2,   0,\n         0,   1,   0, 995,   0,   0,   1,   1,   0,   0,   1,  11,   0,\n         0, 355,   0,   0,   7,   2,   1,   0,   0,   6,   0,   0, 990,\n         0,   1,   0,   1,   8,  16,   1,   2,   0,   2, 898,   1,   1,\n         0,   1,   2,   0,   0,   0,   7,  12, 977])}\nAccuracy: 0.9637\nConfusion Matrix (logged to Azure ML run):\n[[970   0   2   1   4   9   0   2   6   6]\n [  0 425   0   3   0   0   0   0   0   1]\n [  0   0 872  47   0   1   0   0   0  80]\n [  1   0   0 988   1   3   0   0   1   6]\n [  3   0   0  26 945   3   0  22   0   1]\n [  2   0   0   1   0 995   0   0   1   1]\n [  0   0   1  11   0   0 355   0   0   7]\n [  2   1   0   0   6   0   0 990   0   1]\n [  0   1   8  16   1   2   0   2 898   1]\n [  1   0   1   2   0   0   0   7  12 977]]\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1753703776753
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "# Register model (in a new cell after evaluation)\n",
        "from azureml.core import Model\n",
        "model = Model.register(workspace=Workspace.from_config(),\n",
        "                      model_path='model.pth',\n",
        "                      model_name='urban8k_cnn')\n",
        "print(f\"Model registered: {model.name} version: {model.version}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model urban8k_cnn\nModel registered: urban8k_cnn version: 1\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1753704056774
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}